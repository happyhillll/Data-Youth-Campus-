{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "í…ìŠ¤íŠ¸ìƒì„±_ë¸ŒëŸ°ì¹˜í•™ìŠµ(ë¯¸ì™„ì„±).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/happyhillll/-/blob/main/%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%83%9D%EC%84%B1_%EB%B8%8C%EB%9F%B0%EC%B9%98%ED%95%99%EC%8A%B5(%EB%AF%B8%EC%99%84%EC%84%B1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xDktj8noFr2",
        "outputId": "36438c39-feab-4255-9efe-539aa79d611e"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.6 MB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 895 kB 57.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3 MB 26.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 636 kB 63.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT3h3HX4u3sP"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def cleaning(s):\n",
        "  s = str(s)\n",
        "  s = re.sub(', ', ',', s)\n",
        "  s = re.sub(',', ' ', s)\n",
        "  return s\n",
        "\n",
        "df = pd.read_csv(\"/content/prep_brunch_df_France.csv\", encoding=\"utf-8\")\n",
        "df = df.dropna()\n",
        "text_data = open('/content/brunch.txt', 'w', encoding=\"utf-8\")\n",
        "for idx, item in df.iterrows():\n",
        "  article = cleaning(item['Title']) + ', ' + item['Article']+ '\\n'\n",
        "  text_data.write(article)\n",
        "text_data.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrYKwjRMnzt8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "731d450a-3ad1-4bf6-fdd8-e5ef24ab8e14"
      },
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, \n",
        "        mlm=mlm,\n",
        "    )\n",
        "    return data_collator\n",
        "\n",
        "\n",
        "def train(train_file_path,model_name,\n",
        "          output_dir,\n",
        "          overwrite_output_dir,\n",
        "          per_device_train_batch_size,\n",
        "          num_train_epochs,\n",
        "          save_steps):\n",
        "  tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
        "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "  data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "  tokenizer.save_pretrained(output_dir, legacy_format=False)\n",
        "   \n",
        "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "  model.save_pretrained(output_dir)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "          output_dir=output_dir,\n",
        "          overwrite_output_dir=overwrite_output_dir,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          num_train_epochs=num_train_epochs,\n",
        "      )\n",
        "\n",
        "  trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "  )\n",
        "      \n",
        "  trainer.train()\n",
        "  trainer.save_model()\n",
        "\n",
        "\n",
        "train_file_path = '/content/brunch.txt'\n",
        "model_name = 'skt/kogpt2-base-v2'\n",
        "output_dir = './models2'\n",
        "overwrite_output_dir = False\n",
        "per_device_train_batch_size = 8\n",
        "num_train_epochs = 0.5\n",
        "save_steps = 500\n",
        "\n",
        "train(\n",
        "    train_file_path=train_file_path,\n",
        "    model_name=model_name,\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=overwrite_output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    save_steps=save_steps\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 2840\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 178\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='178' max='178' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [178/178 30:10, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to ./models2\n",
            "Configuration saved in ./models2/config.json\n",
            "Model weights saved in ./models2/pytorch_model.bin\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQdvLpFgpydA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84540091-c606-445a-91f7-a456bb7d0029"
      },
      "source": [
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
        "\n",
        "def load_model(model_path):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_tokenizer(tokenizer_path):\n",
        "    tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def generate_text(sequence, max_length):\n",
        "    model_path = \"./models2\"\n",
        "    model = load_model(model_path)\n",
        "    tokenizer = load_tokenizer(model_path)\n",
        "    ids = tokenizer.encode(f'{sequence},', return_tensors='pt')\n",
        "    final_outputs = model.generate(\n",
        "        ids,\n",
        "        do_sample=True,\n",
        "        max_length=max_length,\n",
        "        pad_token_id=model.config.pad_token_id,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "    )\n",
        "    return(tokenizer.decode(final_outputs[0], skip_special_tokens=True))\n",
        "\n",
        "Title = \"í”„ë‘ìŠ¤ í•œë‹¬ì‚´ì´\" #(ìµœëŒ€ 10ê¸€ì)\n",
        "article = \"ë¯¸ì‹ì˜ ë‚˜ë¼\" #(ìµœëŒ€ ê¸€ì ì§€ì •ì˜ˆì •)\n",
        "\n",
        "input = Title + ', ' + article\n",
        "\n",
        "sequence = input\n",
        "max_len = 50\n",
        "\n",
        "print('input :' + sequence)\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('=' * 50)\n",
        "    g = generate_text(sequence, max_len)\n",
        "    g = g.split('<eos>')[0]\n",
        "    g = g.split(',')[2:]\n",
        "    g = ', '.join(g)\n",
        "    print(g)\n",
        "    \n",
        "print('=' * 50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading configuration file ./models2/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.9.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file ./models2/pytorch_model.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "input :í”„ë‘ìŠ¤ í•œë‹¬ì‚´ì´, ë¯¸ì‹ì˜ ë‚˜ë¼\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file ./models2/added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file ./models2/special_tokens_map.json\n",
            "loading file ./models2/tokenizer_config.json\n",
            "loading file ./models2/tokenizer.json\n",
            "loading configuration file ./models2/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.9.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file ./models2/pytorch_model.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " í”„ë‘ìŠ¤ì—ì„œëŠ” 'í”„ë‘ìŠ¤ 3ëŒ€ í•´ì‚°ë¬¼ ìš”ë¦¬'      íŒŒë¦¬ì—ì„œëŠ” ì§€ì¤‘í•´ì‹ í•´ì‚°ë¬¼ ìš”ë¦¬ (Mais-Way-Like)           \n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file ./models2/added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file ./models2/special_tokens_map.json\n",
            "loading file ./models2/tokenizer_config.json\n",
            "loading file ./models2/tokenizer.json\n",
            "loading configuration file ./models2/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.9.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file ./models2/pytorch_model.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " ì´íƒˆë¦¬ì•„.\n",
            "ì´ê²ƒì„ ë³´ë‹ˆ ì–´ëŠìƒˆ í•œêµ­ì— ê°€ê³  ì‹¶ì§€ ì•Šë‹¤ëŠ” ìƒê°ì´ ë“œëŠ”,  ì´ ì—¬í–‰ì€ ì–´ë–¨ê¹Œ?                   \n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file ./models2/added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file ./models2/special_tokens_map.json\n",
            "loading file ./models2/tokenizer_config.json\n",
            "loading file ./models2/tokenizer.json\n",
            "loading configuration file ./models2/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.9.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file ./models2/pytorch_model.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                                         \n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file ./models2/added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file ./models2/special_tokens_map.json\n",
            "loading file ./models2/tokenizer_config.json\n",
            "loading file ./models2/tokenizer.json\n",
            "loading configuration file ./models2/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.9.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file ./models2/pytorch_model.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " í”„ë‘ìŠ¤ëŠ” ê·¸ ë¬´ì—‡ë³´ë‹¤ ë§›ìˆì—ˆë‹¤. ì´ë²ˆì— í”„ë‘ìŠ¤ì—ì„œ ë¨¹ì–´ë³¸ í”„ë‘ìŠ¤ ìš”ë¦¬ ì¤‘ì—ì„œ ë‹¨ì—° ë‹ë³´ì´ëŠ” ìš”ë¦¬ë‹¤.             í”„ë‘ìŠ¤ ë…¸ë¥´ë§ë””ì˜ ëª½ìƒë¯¸ì…¸ì„ ê±°ì³ ë³´ë¥´ë„ë¡œ\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file ./models2/added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file ./models2/special_tokens_map.json\n",
            "loading file ./models2/tokenizer_config.json\n",
            "loading file ./models2/tokenizer.json\n",
            "loading configuration file ./models2/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.9.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file ./models2/pytorch_model.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " í”„ë‘ìŠ¤ í•œë‹¬ì‚´ì´ê°€ ì‹œì‘ëë‹¤. ê·¸ë¦¬ê³  ì—¬ë¦„ë°©í•™ì´ ì‹œì‘ëë‹¤. íŒŒë¦¬ìƒí™œë„ ê½¤ë‚˜ ì–´ë ¤ì› ë‹¤. ì—¬ë¦„ë°©í•™ì´ ì‹œì‘ë˜ìë§ˆì ì—¬ëŸ¬ì°¨ë¡€ íŒŒë¦¬í–‰ í•­ê³µí¸ ì¢Œì„ì´ ëŠê²¨ ì´í‹€ì„ ê¸°ë‹¤\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file ./models2/added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file ./models2/special_tokens_map.json\n",
            "loading file ./models2/tokenizer_config.json\n",
            "loading file ./models2/tokenizer.json\n",
            "loading configuration file ./models2/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.9.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file ./models2/pytorch_model.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " #í”„ë‘ìŠ¤ #í”„ë‘ìŠ¤ #í”„ë‘ìŠ¤ #í”„ë‘ìŠ¤ #íŒŒë¦¬ #í”„ë‘ìŠ¤ì˜ #ë‚¨í”„ë‘ìŠ¤ #í”„ë‘ìŠ¤ #í”„ë‘ìŠ¤ #í”„ë‘ìŠ¤ #íŒŒë¦¬,  #íŒŒë¦¬,  \n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file ./models2/added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file ./models2/special_tokens_map.json\n",
            "loading file ./models2/tokenizer_config.json\n",
            "loading file ./models2/tokenizer.json\n",
            "loading configuration file ./models2/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.9.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file ./models2/pytorch_model.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  íŒŒë¦¬.     íŒŒë¦¬ ê´€ê´‘                                \n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file ./models2/added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file ./models2/special_tokens_map.json\n",
            "loading file ./models2/tokenizer_config.json\n",
            "loading file ./models2/tokenizer.json\n",
            "loading configuration file ./models2/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.9.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file ./models2/pytorch_model.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " ê·¸ë¦¬ê³  í”„ë‘ìŠ¤ ì‚¬ëŒë“¤ì˜ ì—¬ë¦„ì´ ì£¼ëŠ” ê°€ì¥ ì•„ë¦„ë‹¤ìš´ ìˆœê°„ì´ì—ˆë‹¤.     íŒŒë¦¬ì˜ ì—¬ë¦„ì€ í”„ë‘ìŠ¤ì¸ë“¤ì´ ê°€ì¥ ì¢‹ì•„í• ë§Œí•œ ê³„ì ˆ,  ì—í íƒ‘ì´ ì„œ ìˆëŠ” ë£¨ë¸Œë¥´ ë°•ë¬¼ê´€ìœ¼ë¡œ ì¶œë°œí•˜ì˜€ë‹¤. ë¯¸ì‹ìœ¼ë¡œëŠ” ê°€ì¥ ì‚¬ë‘ë°›ëŠ” í”„ë‘ìŠ¤\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file ./models2/added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file ./models2/special_tokens_map.json\n",
            "loading file ./models2/tokenizer_config.json\n",
            "loading file ./models2/tokenizer.json\n",
            "loading configuration file ./models2/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
            "  \"bos_token_id\": 0,\n",
            "  \"created_date\": \"2021-04-28\",\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.9.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "loading weights file ./models2/pytorch_model.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " íŒŒë¦¬. ê·¸ëŸ°ë° ì´ì œ  ì™ ì§€ ì—¬í–‰ì€ í¬ê¸°í•˜ê³ ,  ì—¬í–‰ì€ í•œë‚˜ì ˆì„ ëª½ë•… í¬ê¸°í•´ì•¼ í•˜ëŠ” ë‚˜ë‚ ì—. í•œ ë‹¬ì‚´ì´ ì™ ì§€ ì—¬ìœ ê°€ ì—†ì„ ê²ƒ ê°™ë‹¤ëŠ” ìƒê°ì´ ë“œëŠ”ë° ê·¸ë˜ë„ ë‹¤í–‰\n",
            "==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./models2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Didn't find file ./models2/added_tokens.json. We won't load it.\n",
            "loading file None\n",
            "loading file ./models2/special_tokens_map.json\n",
            "loading file ./models2/tokenizer_config.json\n",
            "loading file ./models2/tokenizer.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " í”„ë‘ìŠ¤ëŠ” ì‰ì´í¬ë¼ëŠ” ë‹¨ì–´ê°€ ë” ì´ìƒ ì…ì— ë§ì§€ ì•ŠëŠ”ë‹¤.        í”„ë‘ìŠ¤  ì‰ì´í¬ëŠ” ì›ë˜ í”„ë‘ìŠ¤ì—ì„œ ê°€ì¥ í° ë ˆìŠ¤í† ë‘ì´ë‹¤. í”„ë‘ìŠ¤ ë ˆìŠ¤í† ë‘ë“¤ì€ ê³ ê¸‰ìŠ¤ëŸ¬ìš´ ë¶„ìœ„ê¸°ì˜ ì‹\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGsAumzg5Lrf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}